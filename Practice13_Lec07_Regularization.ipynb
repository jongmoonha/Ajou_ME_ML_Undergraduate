{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice 13 — Regularization\n",
    "\n",
    "Original Authors: Seungjae Ryan Lee, Ki Hyun Kim\n",
    "Source: https://github.com/deeplearningzerotoall/PyTorch\n",
    "\n",
    "모델 용량이 클수록 **과적합 위험**이 높아집니다. 규제(Regularization)로 일반화 성능을 향상시킵니다.\n",
    "\n",
    "| 기법 | 핵심 |\n",
    "|------|------|\n",
    "| L2 (Weight Decay) | 가중치 크기에 벌칙 → 작은 가중치 유지 |\n",
    "| L1 (Lasso) | 가중치를 0으로 → 희소 모델 |\n",
    "| Dropout | 랜덤 노드 제거 → 앙상블 효과 |"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# 과적합을 유발하기 위해 학습 데이터를 10%만 사용 (179개)\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.9, random_state=42, stratify=y)\n",
    "\n",
    "mu = X_train.mean(0)\n",
    "sigma = X_train.std(0) + 1e-8\n",
    "X_train_s = (X_train - mu) / sigma\n",
    "X_test_s = (X_test - mu) / sigma\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(torch.FloatTensor(X_train_s), torch.LongTensor(y_train)),\n",
    "    batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(\n",
    "    TensorDataset(torch.FloatTensor(X_test_s), torch.LongTensor(y_test)),\n",
    "    batch_size=256)\n",
    "\n",
    "print(f'Train: {len(y_train)}, Test: {len(y_test)}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 모델을 학습하고 epoch별 (train_loss, test_loss, test_acc) 리스트를 반환합니다.\n",
    "def train(model, train_loader, test_loader, optimizer, epochs=150):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    test_accs = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # 훈련\n",
    "        model.train()\n",
    "        loss_sum = 0\n",
    "        count = 0\n",
    "        for X_batch, Y_batch in train_loader:\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, Y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_sum += loss.item() * len(Y_batch)\n",
    "            count += len(Y_batch)\n",
    "        train_losses.append(loss_sum / count)\n",
    "\n",
    "        # 테스트\n",
    "        model.eval()\n",
    "        loss_sum = 0\n",
    "        correct = 0\n",
    "        count = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, Y_batch in test_loader:\n",
    "                output = model(X_batch)\n",
    "                loss_sum += criterion(output, Y_batch).item() * len(Y_batch)\n",
    "                correct += (output.argmax(1) == Y_batch).sum().item()\n",
    "                count += len(Y_batch)\n",
    "        test_losses.append(loss_sum / count)\n",
    "        test_accs.append(correct / count)\n",
    "\n",
    "    return train_losses, test_losses, test_accs"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. L2 규제 (Weight Decay)\n",
    "\n",
    "가중치 크기의 **제곱합**에 벌칙을 부여합니다.\n",
    "\n",
    "$$J_{\\text{reg}} = J(\\theta) + \\lambda \\|\\theta\\|_2^2$$\n",
    "\n",
    "PyTorch에서는 optimizer의 `weight_decay` 파라미터로 간단히 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# === 규제 없음 (Baseline) ===\n",
    "torch.manual_seed(42)\n",
    "model_base = nn.Sequential(\n",
    "    nn.Linear(64, 256), nn.ReLU(),\n",
    "    nn.Linear(256, 256), nn.ReLU(),\n",
    "    nn.Linear(256, 256), nn.ReLU(),\n",
    "    nn.Linear(256, 10)\n",
    ")\n",
    "for layer in model_base:  # He 초기화 (Practice 12에서 배운 기법)\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        nn.init.kaiming_normal_(layer.weight, nonlinearity='relu')\n",
    "optimizer_base = torch.optim.SGD(model_base.parameters(), lr=0.01, momentum=0.9)\n",
    "tl_base, vl_base, va_base = train(model_base, train_loader, test_loader, optimizer_base)\n",
    "\n",
    "# === L2 규제 (weight_decay=0.01) ===\n",
    "torch.manual_seed(42)\n",
    "model_l2 = nn.Sequential(\n",
    "    nn.Linear(64, 256), nn.ReLU(),\n",
    "    nn.Linear(256, 256), nn.ReLU(),\n",
    "    nn.Linear(256, 256), nn.ReLU(),\n",
    "    nn.Linear(256, 10)\n",
    ")\n",
    "for layer in model_l2:\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        nn.init.kaiming_normal_(layer.weight, nonlinearity='relu')\n",
    "optimizer_l2 = torch.optim.SGD(model_l2.parameters(), lr=0.01, momentum=0.9, weight_decay=0.01)\n",
    "tl_l2, vl_l2, va_l2 = train(model_l2, train_loader, test_loader, optimizer_l2)\n",
    "\n",
    "print(f'No Reg -> Test Acc: {va_base[-1]:.3f}')\n",
    "print(f'L2     -> Test Acc: {va_l2[-1]:.3f}')\n",
    "\n",
    "# 비교 그래프\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "axes[0].plot(vl_base, label='No Reg')\n",
    "axes[0].plot(vl_l2, label='L2 (wd=0.01)')\n",
    "axes[0].set_xlabel('Epoch'); axes[0].set_ylabel('Test Loss')\n",
    "axes[0].set_title('L2 Regularization - Test Loss')\n",
    "axes[0].legend(); axes[0].grid(alpha=0.3)\n",
    "axes[1].plot(va_base, label='No Reg')\n",
    "axes[1].plot(va_l2, label='L2 (wd=0.01)')\n",
    "axes[1].set_xlabel('Epoch'); axes[1].set_ylabel('Test Accuracy')\n",
    "axes[1].set_title('L2 Regularization - Test Accuracy')\n",
    "axes[1].legend(); axes[1].grid(alpha=0.3)\n",
    "plt.tight_layout(); plt.show()\n",
    "print('L2는 가중치 크기를 제한하여 과적합을 줄입니다.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. L1 규제와 가중치 분포\n",
    "\n",
    "L1 규제는 가중치를 **정확히 0으로** 만드는 경향이 있습니다 (희소성).\n",
    "\n",
    "PyTorch에는 L1이 내장되어 있지 않으므로 수동으로 구현합니다.\n",
    "\n",
    "$$J_{\\text{reg}} = J(\\theta) + \\lambda \\|\\theta\\|_1$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# === L1 규제 (직접 구현) ===\n",
    "torch.manual_seed(42)\n",
    "model_l1 = nn.Sequential(\n",
    "    nn.Linear(64, 256), nn.ReLU(),\n",
    "    nn.Linear(256, 256), nn.ReLU(),\n",
    "    nn.Linear(256, 256), nn.ReLU(),\n",
    "    nn.Linear(256, 10)\n",
    ")\n",
    "for layer in model_l1:\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        nn.init.kaiming_normal_(layer.weight, nonlinearity='relu')\n",
    "optimizer_l1 = torch.optim.SGD(model_l1.parameters(), lr=0.01, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "l1_lambda = 5e-4\n",
    "\n",
    "# L1은 loss에 가중치 절대값 합을 직접 더해줍니다\n",
    "for epoch in range(150):\n",
    "    model_l1.train()\n",
    "    for X_batch, Y_batch in train_loader:\n",
    "        output = model_l1(X_batch)\n",
    "        ce_loss = criterion(output, Y_batch)\n",
    "        l1_penalty = sum(p.abs().sum() for p in model_l1.parameters())\n",
    "        loss = ce_loss + l1_lambda * l1_penalty\n",
    "        optimizer_l1.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_l1.step()\n",
    "\n",
    "# 가중치 분포 비교 (No Reg vs L2 vs L1)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 3))\n",
    "\n",
    "w_base = torch.cat([p.data.flatten() for p in model_base.parameters()]).numpy()\n",
    "axes[0].hist(w_base, bins=50, alpha=0.7, edgecolor='black', linewidth=0.5)\n",
    "axes[0].set_title('No Reg'); axes[0].set_xlabel('Weight')\n",
    "near_zero = (np.abs(w_base) < 0.01).mean()\n",
    "axes[0].text(0.95, 0.95, f'|w|<0.01: {near_zero:.1%}', transform=axes[0].transAxes,\n",
    "             ha='right', va='top', fontsize=9, bbox=dict(facecolor='wheat', alpha=0.5))\n",
    "\n",
    "w_l2 = torch.cat([p.data.flatten() for p in model_l2.parameters()]).numpy()\n",
    "axes[1].hist(w_l2, bins=50, alpha=0.7, edgecolor='black', linewidth=0.5)\n",
    "axes[1].set_title('L2 (wd=0.01)'); axes[1].set_xlabel('Weight')\n",
    "near_zero = (np.abs(w_l2) < 0.01).mean()\n",
    "axes[1].text(0.95, 0.95, f'|w|<0.01: {near_zero:.1%}', transform=axes[1].transAxes,\n",
    "             ha='right', va='top', fontsize=9, bbox=dict(facecolor='wheat', alpha=0.5))\n",
    "\n",
    "w_l1 = torch.cat([p.data.flatten() for p in model_l1.parameters()]).numpy()\n",
    "axes[2].hist(w_l1, bins=50, alpha=0.7, edgecolor='black', linewidth=0.5)\n",
    "axes[2].set_title('L1 (lambda=5e-4)'); axes[2].set_xlabel('Weight')\n",
    "near_zero = (np.abs(w_l1) < 0.01).mean()\n",
    "axes[2].text(0.95, 0.95, f'|w|<0.01: {near_zero:.1%}', transform=axes[2].transAxes,\n",
    "             ha='right', va='top', fontsize=9, bbox=dict(facecolor='wheat', alpha=0.5))\n",
    "\n",
    "for ax in axes:\n",
    "    ax.grid(alpha=0.3)\n",
    "plt.suptitle('Weight Distribution Comparison', fontsize=13)\n",
    "plt.tight_layout(); plt.show()\n",
    "print('L1은 많은 가중치를 0으로 만들어 희소(sparse) 모델을 생성합니다.')\n",
    "print('L2는 전체 가중치를 균일하게 작게 유지합니다.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. 드롭아웃 (Dropout)\n",
    "\n",
    "훈련 시 매 iteration마다 **랜덤으로 노드를 비활성화** → 암묵적 앙상블 효과\n",
    "\n",
    "- `nn.Dropout(p)`: p = 비활성화 확률\n",
    "- `model.train()` → 드롭아웃 활성\n",
    "- `model.eval()` → 드롭아웃 비활성 (전체 노드 사용)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# === Dropout 없음 ===\n",
    "torch.manual_seed(42)\n",
    "model_no_drop = nn.Sequential(\n",
    "    nn.Linear(64, 256), nn.ReLU(),\n",
    "    nn.Linear(256, 256), nn.ReLU(),\n",
    "    nn.Linear(256, 256), nn.ReLU(),\n",
    "    nn.Linear(256, 10)\n",
    ")\n",
    "for layer in model_no_drop:\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        nn.init.kaiming_normal_(layer.weight, nonlinearity='relu')\n",
    "optimizer_no_drop = torch.optim.SGD(model_no_drop.parameters(), lr=0.01, momentum=0.9)\n",
    "tl_no, vl_no, va_no = train(model_no_drop, train_loader, test_loader, optimizer_no_drop)\n",
    "\n",
    "# === Dropout 0.3 ===\n",
    "torch.manual_seed(42)\n",
    "model_drop = nn.Sequential(\n",
    "    nn.Linear(64, 256), nn.ReLU(), nn.Dropout(0.3),\n",
    "    nn.Linear(256, 256), nn.ReLU(), nn.Dropout(0.3),\n",
    "    nn.Linear(256, 256), nn.ReLU(), nn.Dropout(0.3),\n",
    "    nn.Linear(256, 10)\n",
    ")\n",
    "for layer in model_drop:\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        nn.init.kaiming_normal_(layer.weight, nonlinearity='relu')\n",
    "optimizer_drop = torch.optim.SGD(model_drop.parameters(), lr=0.01, momentum=0.9)\n",
    "tl_drop, vl_drop, va_drop = train(model_drop, train_loader, test_loader, optimizer_drop)\n",
    "\n",
    "print(f'No Dropout  -> Test Acc: {va_no[-1]:.3f}')\n",
    "print(f'Dropout 0.3 -> Test Acc: {va_drop[-1]:.3f}')\n",
    "\n",
    "# 비교 그래프\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "axes[0].plot(vl_no, label='No Dropout')\n",
    "axes[0].plot(vl_drop, label='Dropout 0.3')\n",
    "axes[0].set_xlabel('Epoch'); axes[0].set_ylabel('Test Loss')\n",
    "axes[0].set_title('Dropout - Test Loss')\n",
    "axes[0].legend(); axes[0].grid(alpha=0.3)\n",
    "axes[1].plot(va_no, label='No Dropout')\n",
    "axes[1].plot(va_drop, label='Dropout 0.3')\n",
    "axes[1].set_xlabel('Epoch'); axes[1].set_ylabel('Test Accuracy')\n",
    "axes[1].set_title('Dropout - Test Accuracy')\n",
    "axes[1].legend(); axes[1].grid(alpha=0.3)\n",
    "plt.tight_layout(); plt.show()\n",
    "print('Dropout은 과적합을 줄여 일반화 성능을 향상시킵니다.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. 하이퍼파라미터 탐색 (Hyperparameter Search)\n",
    "\n",
    "규제 강도, 학습률 등은 **하이퍼파라미터**로, 별도의 탐색이 필요합니다.\n",
    "\n",
    "| 방법 | 장점 | 단점 |\n",
    "|------|------|------|\n",
    "| Grid Search | 체계적, 재현 가능 | 조합 폭발 ($q^m$) |\n",
    "| Random Search | 중요 파라미터에 유리 | 비체계적 |"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Learning Rate x Weight Decay Grid Search\n",
    "lr_values = [0.001, 0.01, 0.05]\n",
    "wd_values = [0.0, 0.001, 0.01, 0.1]\n",
    "grid = np.zeros((len(lr_values), len(wd_values)))\n",
    "\n",
    "for i, lr in enumerate(lr_values):\n",
    "    for j, wd in enumerate(wd_values):\n",
    "        torch.manual_seed(42)\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(64, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 10))\n",
    "        _, _, va = train(model, train_loader, test_loader,\n",
    "                         torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd),\n",
    "                         epochs=80)\n",
    "        grid[i, j] = max(va)\n",
    "\n",
    "# 히트맵\n",
    "fig, ax = plt.subplots(figsize=(7, 3.5))\n",
    "im = ax.imshow(grid, cmap='YlGn', aspect='auto', vmin=grid.min() - 0.02, vmax=grid.max())\n",
    "ax.set_xticks(range(len(wd_values)))\n",
    "ax.set_xticklabels(wd_values)\n",
    "ax.set_yticks(range(len(lr_values)))\n",
    "ax.set_yticklabels(lr_values)\n",
    "ax.set_xlabel('Weight Decay')\n",
    "ax.set_ylabel('Learning Rate')\n",
    "ax.set_title('Grid Search - Best Test Accuracy')\n",
    "for i in range(len(lr_values)):\n",
    "    for j in range(len(wd_values)):\n",
    "        ax.text(j, i, f'{grid[i,j]:.3f}', ha='center', va='center', fontweight='bold')\n",
    "plt.colorbar(im)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "best = np.unravel_index(grid.argmax(), grid.shape)\n",
    "print(f'Best: lr={lr_values[best[0]]}, wd={wd_values[best[1]]} -> Acc: {grid[best]:.3f}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 연습 문제\n",
    "\n",
    "1. L2 규제 강도(`weight_decay`)를 0.001, 0.01, 0.1로 변화시키며 학습 곡선을 비교하세요.\n",
    "2. Dropout을 0.1, 0.3, 0.5로 바꿀 때 테스트 정확도가 어떻게 달라지는지 비교하세요."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 연습 문제 풀이 공간\n",
    ""
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}
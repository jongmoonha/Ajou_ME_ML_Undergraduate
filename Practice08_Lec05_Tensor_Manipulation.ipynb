{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice 08 — Tensor Manipulation\n",
    "\n",
    "Original Authors: Seungjae Ryan Lee, Ki Hyun Kim  \n",
    "Source: https://github.com/deeplearningzerotoall/PyTorch\n",
    "\n",
    "Practice 07에서 Tensor 생성과 Autograd를 배웠습니다.  \n",
    "이 노트북에서는 PyTorch Tensor의 **조작(manipulation)** 연산을 익힙니다.\n",
    "\n",
    "| 핵심 개념 | 설명 |\n",
    "|---|---|\n",
    "| `view` / `reshape` | 텐서 형태 변환 (NumPy의 `reshape`에 해당) |\n",
    "| `matmul` / `@` | 행렬 곱셈 |\n",
    "| `squeeze` / `unsqueeze` | 차원 축소/확장 |\n",
    "| `cat` / `stack` | 텐서 결합 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. NumPy → PyTorch\n",
    "\n",
    "PyTorch 텐서는 NumPy 배열과 거의 동일한 API를 가지고 있습니다.  \n",
    "NumPy에서 익숙한 연산들이 PyTorch에서도 같은 방식으로 동작합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.FloatTensor([0., 1., 2., 3., 4., 5., 6.])\n",
    "print(t)\n",
    "print(f'dim : {t.dim()}')     # rank (차원 수)\n",
    "print(f'shape: {t.shape}')    # shape\n",
    "print(f'size : {t.size()}')   # shape과 동일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱싱과 슬라이싱 — NumPy와 동일\n",
    "print('t[0], t[1], t[-1] =', t[0].item(), t[1].item(), t[-1].item())\n",
    "print('t[2:5]  =', t[2:5])     # 인덱스 2~4\n",
    "print('t[:2]   =', t[:2])      # 처음~1\n",
    "print('t[3:]   =', t[3:])      # 3~끝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.FloatTensor([[1., 2., 3.],\n",
    "                        [4., 5., 6.],\n",
    "                        [7., 8., 9.],\n",
    "                        [10., 11., 12.]])\n",
    "print(t)\n",
    "print(f'dim : {t.dim()}')      # 2\n",
    "print(f'size: {t.size()}')     # (4, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D 슬라이싱\n",
    "print('2번째 열:', t[:, 1])          # 모든 행의 2번째 열\n",
    "print('마지막 열 제외:', t[:, :-1])   # 모든 행에서 마지막 열 제외"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape, Rank, Axis\n",
    "\n",
    "| 용어 | 의미 | 예시 |\n",
    "|---|---|---|\n",
    "| **Rank** | 차원 수 (`dim()`) | 2D 텐서 = rank 2 |\n",
    "| **Shape** | 각 차원의 크기 | `(4, 3)` = 4행 3열 |\n",
    "| **Axis** | 특정 차원 방향 | `dim=0` (행 방향), `dim=1` (열 방향) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 고차원 텐서 — 이미지 배치를 표현할 때 사용\n",
    "# (batch, channel, height, width)\n",
    "t = torch.FloatTensor([[[[1, 2, 3, 4],\n",
    "                          [5, 6, 7, 8],\n",
    "                          [9, 10, 11, 12]],\n",
    "                         [[13, 14, 15, 16],\n",
    "                          [17, 18, 19, 20],\n",
    "                          [21, 22, 23, 24]]]])\n",
    "\n",
    "print(f'dim : {t.dim()}')      # rank = 4\n",
    "print(f'size: {t.size()}')     # (1, 2, 3, 4) — batch=1, channel=2, H=3, W=4"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### PyTorch ↔ NumPy 변환\n\n| 메서드 | 설명 |\n|---|---|\n| `torch.tensor(arr)` | NumPy 배열 → 텐서 (복사) |\n| `torch.from_numpy(arr)` | NumPy 배열 → 텐서 (**메모리 공유**) |\n| `.numpy()` | 텐서 → NumPy 배열 (`requires_grad=True`이면 `.detach()` 먼저) |\n| `.item()` | 단일 원소 텐서 → Python 숫자 |\n| `.detach()` | autograd 그래프에서 분리한 새 텐서 반환 |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# .item() — 단일 원소 텐서를 Python 숫자로 변환\nt = torch.tensor(3.14)\nprint(f'item(): {t.item()},  type: {type(t.item()).__name__}')\n\n# .detach() — autograd 그래프에서 분리\nt_grad = torch.tensor(2.0, requires_grad=True)\nt_detached = t_grad.detach()\nprint(f'\\nrequires_grad: {t_grad.requires_grad}  →  detach: {t_detached.requires_grad}')\n\n# .detach().numpy() — grad 추적 중인 Tensor → NumPy\narr = t_grad.detach().numpy()\nprint(f'detach().numpy(): {arr},  type: {type(arr).__name__}')\n\n# torch.from_numpy() — NumPy → Tensor (메모리 공유: 한쪽 수정 시 다른 쪽도 변경)\nnp_arr = np.array([1.0, 2.0, 3.0])\nt_shared = torch.from_numpy(np_arr)\nprint(f'\\nfrom_numpy: {t_shared}')\nnp_arr[0] = 999.0\nprint(f'NumPy 수정 후 Tensor도 변경: {t_shared}')   # 메모리 공유 확인",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. 텐서 연산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mul vs Matmul\n",
    "\n",
    "Practice 01에서 배운 `*` (element-wise)와 `@` (행렬 곱셈)의 차이가 PyTorch에서도 동일하게 적용됩니다.\n",
    "\n",
    "| 연산 | 의미 | PyTorch |\n",
    "|---|---|---|\n",
    "| 원소별 곱셈 | $a_{ij} \\times b_{ij}$ | `*`, `.mul()` |\n",
    "| 행렬 곱셈 | $\\sum_k a_{ik} b_{kj}$ | `@`, `.matmul()` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "m2 = torch.FloatTensor([[1], [2]])\n",
    "print('Shape of m1:', m1.shape)   # (2, 2)\n",
    "print('Shape of m2:', m2.shape)   # (2, 1)\n",
    "\n",
    "# 행렬 곱셈: (2,2) @ (2,1) = (2,1)\n",
    "print('\\nMatmul (m1 @ m2):')\n",
    "print(m1.matmul(m2))\n",
    "\n",
    "# 원소별 곱셈: broadcasting으로 (2,2) * (2,1) = (2,2)\n",
    "print('\\nElement-wise (m1 * m2):')\n",
    "print(m1 * m2)\n",
    "print(m1.mul(m2))               # 동일"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting\n",
    "\n",
    "크기가 다른 텐서끼리 연산할 때, PyTorch가 자동으로 크기를 맞춰줍니다.\n",
    "\n",
    "> **주의:** Broadcasting은 편리하지만, 의도하지 않은 결과를 낼 수 있습니다.  \n",
    "> shape을 항상 확인하는 습관이 중요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 같은 shape — broadcasting 없음\n",
    "m1 = torch.FloatTensor([[3, 3]])\n",
    "m2 = torch.FloatTensor([[2, 2]])\n",
    "print('Same shape:', m1 + m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector + Scalar: [3] -> [[3, 3]] 으로 확장\n",
    "m1 = torch.FloatTensor([[1, 2]])\n",
    "m2 = torch.FloatTensor([3])\n",
    "print('Vector + Scalar:', m1 + m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1,2) + (2,1) -> (2,2)  — 양쪽 모두 확장\n",
    "m1 = torch.FloatTensor([[1, 2]])       # (1, 2)\n",
    "m2 = torch.FloatTensor([[3], [4]])     # (2, 1)\n",
    "print(m1 + m2)\n",
    "# [[1,2]] + [[3],    [[1+3, 2+3],    [[4, 5],\n",
    "#            [4]]  =  [1+4, 2+4]]  =  [5, 6]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. 축(dim)별 집계\n",
    "\n",
    "NumPy의 `axis`와 동일한 개념입니다.  \n",
    "`dim=0`은 **행 방향(위→아래)**으로 집계, `dim=1`은 **열 방향(왼→오)**으로 집계합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "print(t)\n",
    "print(f'\\nmean()      = {t.mean()}')         # 전체 평균: 2.5\n",
    "print(f'mean(dim=0) = {t.mean(dim=0)}')      # 열별 평균: [2, 3]\n",
    "print(f'mean(dim=1) = {t.mean(dim=1)}')      # 행별 평균: [1.5, 3.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정수 텐서는 mean() 사용 불가 — float으로 변환 필요\n",
    "t_int = torch.LongTensor([1, 2])\n",
    "try:\n",
    "    print(t_int.mean())\n",
    "except Exception as e:\n",
    "    print(f'Error: {e}')\n",
    "    print(f'Solution: {t_int.float().mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "print(t)\n",
    "print(f'\\nsum()      = {t.sum()}')           # 전체 합: 10\n",
    "print(f'sum(dim=0) = {t.sum(dim=0)}')        # 열별 합: [4, 6]\n",
    "print(f'sum(dim=1) = {t.sum(dim=1)}')        # 행별 합: [3, 7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max / Argmax\n",
    "\n",
    "`max()`에 `dim`을 지정하면 **(최대값, 인덱스)** 두 개를 반환합니다.  \n",
    "`argmax`는 분류 모델에서 **예측 클래스를 선택**할 때 핵심적으로 사용됩니다.\n",
    "\n",
    "```python\n",
    "# 이후 실습에서 자주 볼 코드:\n",
    "pred = logits.argmax(dim=1)  # 각 샘플의 예측 클래스\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "print(t)\n",
    "\n",
    "# dim 없이 — 전체 최대값 1개\n",
    "print(f'\\nmax() = {t.max()}')\n",
    "\n",
    "# dim=0 — 열별 최대값 + 해당 행 인덱스\n",
    "values, indices = t.max(dim=0)\n",
    "print(f'\\nmax(dim=0):')\n",
    "print(f'  values : {values}')     # [3, 4]\n",
    "print(f'  indices: {indices}')    # [1, 1] — 둘 다 행 인덱스 1\n",
    "\n",
    "# dim=1 — 행별 최대값 + 해당 열 인덱스\n",
    "values, indices = t.max(dim=1)\n",
    "print(f'\\nmax(dim=1):')\n",
    "print(f'  values : {values}')     # [2, 4]\n",
    "print(f'  indices: {indices}')    # [1, 1] — 둘 다 열 인덱스 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Shape 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### view — NumPy의 reshape에 해당\n",
    "\n",
    "`view`는 텐서의 **원소 수를 유지**하면서 형태를 바꿉니다.  \n",
    "`-1`을 사용하면 해당 차원의 크기를 **자동 계산**합니다.\n",
    "\n",
    "```python\n",
    "# 이후 실습에서 자주 볼 코드:\n",
    "x = x.view(-1, 784)      # (N, 28, 28) -> (N, 784)  FC 입력\n",
    "y = y.view(-1, 1)        # (N,) -> (N, 1)  열 벡터\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.FloatTensor([[[0, 1, 2],\n",
    "                         [3, 4, 5]],\n",
    "                        [[6, 7, 8],\n",
    "                         [9, 10, 11]]])\n",
    "print(f'Original shape: {t.shape}')    # (2, 2, 3)\n",
    "\n",
    "# (2,2,3) -> (4,3)  — -1은 자동 계산: 12/3 = 4\n",
    "print(f'\\nview(-1, 3):')\n",
    "print(t.view(-1, 3))\n",
    "print(f'shape: {t.view(-1, 3).shape}')\n",
    "\n",
    "# (2,2,3) -> (4,1,3)\n",
    "print(f'\\nview(-1, 1, 3):')\n",
    "print(t.view(-1, 1, 3))\n",
    "print(f'shape: {t.view(-1, 1, 3).shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### squeeze — 크기 1인 차원 제거\n",
    "\n",
    "모델 출력이 `(N, 1)` 형태일 때, `squeeze()`로 `(N,)`으로 만들 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = torch.FloatTensor([[0], [1], [2]])\n",
    "print(f'Original: {ft.shape}')          # (3, 1)\n",
    "print(ft)\n",
    "\n",
    "print(f'\\nSqueeze: {ft.squeeze().shape}')  # (3,)\n",
    "print(ft.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unsqueeze — 차원 추가\n",
    "\n",
    "`squeeze`의 반대 연산입니다. 지정한 위치에 **크기 1인 차원을 삽입**합니다.\n",
    "\n",
    "```python\n",
    "# 이후 실습에서 자주 볼 코드:\n",
    "x = x.unsqueeze(0)     # (C, H, W) -> (1, C, H, W)  배치 차원 추가\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = torch.Tensor([0, 1, 2])\n",
    "print(f'Original: {ft.shape}')               # (3,)\n",
    "\n",
    "# dim=0: 맨 앞에 차원 추가 -> (1, 3)  행 벡터\n",
    "print(f'\\nunsqueeze(0): {ft.unsqueeze(0).shape}')\n",
    "print(ft.unsqueeze(0))\n",
    "\n",
    "# dim=1: 두 번째에 차원 추가 -> (3, 1)  열 벡터\n",
    "print(f'\\nunsqueeze(1): {ft.unsqueeze(1).shape}')\n",
    "print(ft.unsqueeze(1))\n",
    "\n",
    "# view로도 동일한 결과\n",
    "print(f'\\nview(1, -1): {ft.view(1, -1).shape}')   # unsqueeze(0)과 동일\n",
    "print(f'view(-1, 1): {ft.view(-1, 1).shape}')     # unsqueeze(1)과 동일"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. 기타 유용한 연산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type Casting\n",
    "\n",
    "텐서의 자료형을 변환합니다. 모델 입력은 보통 `float`, 레이블은 `long`을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt = torch.LongTensor([1, 2, 3, 4])\n",
    "print(f'LongTensor : {lt}')\n",
    "print(f'.float()   : {lt.float()}')\n",
    "\n",
    "bt = torch.BoolTensor([True, False, False, True])\n",
    "print(f'\\nBoolTensor : {bt}')\n",
    "print(f'.long()    : {bt.long()}')\n",
    "print(f'.float()   : {bt.float()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenation (torch.cat)\n",
    "\n",
    "**기존 차원**을 따라 텐서를 이어 붙입니다.  \n",
    "NumPy의 `np.concatenate`, `np.vstack`, `np.hstack`에 해당합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "y = torch.FloatTensor([[5, 6], [7, 8]])\n",
    "\n",
    "# dim=0: 행 방향으로 이어 붙이기 (= np.vstack)\n",
    "print('cat dim=0 (vstack):')\n",
    "print(torch.cat([x, y], dim=0))\n",
    "\n",
    "# dim=1: 열 방향으로 이어 붙이기 (= np.hstack)\n",
    "print('\\ncat dim=1 (hstack):')\n",
    "print(torch.cat([x, y], dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking (torch.stack)\n",
    "\n",
    "**새로운 차원**을 만들어서 텐서를 쌓습니다.  \n",
    "`cat`과 달리 차원이 하나 증가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor([1, 4])\n",
    "y = torch.FloatTensor([2, 5])\n",
    "z = torch.FloatTensor([3, 6])\n",
    "\n",
    "# 기본 dim=0: 각 텐서가 하나의 행이 됨\n",
    "print('stack dim=0:')\n",
    "print(torch.stack([x, y, z]))\n",
    "print(f'shape: {torch.stack([x, y, z]).shape}')     # (3, 2)\n",
    "\n",
    "# dim=1: 각 텐서가 하나의 열이 됨\n",
    "print('\\nstack dim=1:')\n",
    "print(torch.stack([x, y, z], dim=1))\n",
    "print(f'shape: {torch.stack([x, y, z], dim=1).shape}')  # (2, 3)\n",
    "\n",
    "# stack = unsqueeze + cat 과 동일\n",
    "print('\\ncat + unsqueeze (동일 결과):')\n",
    "print(torch.cat([x.unsqueeze(0), y.unsqueeze(0), z.unsqueeze(0)], dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ones_like / zeros_like\n",
    "\n",
    "기존 텐서와 **같은 shape, 같은 dtype**으로 1 또는 0으로 채운 텐서를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor([[0, 1, 2], [2, 1, 0]])\n",
    "print('x:')\n",
    "print(x)\n",
    "print(f'\\nones_like : {torch.ones_like(x)}')\n",
    "print(f'zeros_like: {torch.zeros_like(x)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-place 연산\n",
    "\n",
    "메서드 이름 뒤에 `_`(언더스코어)를 붙이면 **원본 텐서를 직접 수정**합니다.  \n",
    "`_` 없으면 새 텐서를 반환하고 원본은 그대로입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "\n",
    "# mul(): 새 텐서 반환, 원본 유지\n",
    "print('mul(2):', x.mul(2))\n",
    "print('x (unchanged):', x)\n",
    "\n",
    "# mul_(): 원본 직접 수정 (in-place)\n",
    "print('\\nmul_(2):', x.mul_(2))\n",
    "print('x (changed!):', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot Encoding (scatter)\n",
    "\n",
    "클래스 인덱스를 **one-hot 벡터**로 변환합니다.  \n",
    "예: 클래스 2 (3개 클래스) → `[0, 0, 1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4개 샘플의 클래스 인덱스\n",
    "labels = torch.LongTensor([[0], [1], [2], [0]])\n",
    "print('Labels:', labels.flatten().tolist())\n",
    "\n",
    "# one-hot 변환: (batch_size, num_classes) 크기의 0 텐서에 scatter\n",
    "one_hot = torch.zeros(4, 3)              # 4 samples, 3 classes\n",
    "one_hot.scatter_(1, labels, 1)           # dim=1, 인덱스 위치에 1.0 채움\n",
    "print('\\nOne-hot:')\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 요약\n",
    "\n",
    "| 카테고리 | 연산 | 설명 |\n",
    "|---|---|---|\n",
    "| **생성** | `torch.FloatTensor`, `zeros`, `ones` | 텐서 생성 |\n",
    "| **속성** | `dim()`, `shape`, `size()` | 차원, 형태 조회 |\n",
    "| **연산** | `*` / `@`, `matmul` | 원소별 곱 / 행렬 곱 |\n",
    "| **집계** | `mean`, `sum`, `max` | `dim` 지정으로 축별 집계 |\n",
    "| **변환** | `view`, `squeeze`, `unsqueeze` | shape 변환 |\n",
    "| **결합** | `cat`, `stack` | 기존 차원 / 새 차원으로 결합 |\n",
    "| **기타** | `float()`, `long()`, `_` suffix | 타입 변환, in-place 연산 |\n",
    "\n",
    "> 다음 Practice에서는 이 연산들을 활용하여 **선형 모델**을 PyTorch로 구현합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}